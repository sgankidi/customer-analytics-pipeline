# Customer Analytics Pipeline

This project implements an **ELT pipeline** using **Apache Airflow, dbt, and Snowflake** to extract customer transactions from PostgreSQL, load them into Snowflake, and transform them into analytics-ready data.

## ğŸš€ Tech Stack

- **Apache Airflow** â†’ Orchestrates the ELT process
- **dbt (Data Build Tool)** â†’ Transforms raw data into analytics tables
- **Snowflake** â†’ Cloud data warehouse
- **PostgreSQL** â†’ Source database
- **Docker** â†’ Containerized environment

## ğŸ“ Project Structure

```
customer-analytics-pipeline/
â”‚â”€â”€ airflow/  
â”‚   â”œâ”€â”€ dags/  
â”‚   â”‚   â”œâ”€â”€ elt_pipeline.py  
â”‚   â”œâ”€â”€ docker-compose.yml  
â”‚   â”œâ”€â”€ airflow.cfg  
â”‚â”€â”€ dbt/  
â”‚   â”œâ”€â”€ models/  
â”‚   â”‚   â”œâ”€â”€ staging/  
â”‚   â”‚   â”‚   â”œâ”€â”€ transactions.sql  
â”‚   â”‚   â”œâ”€â”€ marts/  
â”‚   â”‚   â”‚   â”œâ”€â”€ customer_revenue.sql  
â”‚   â”œâ”€â”€ dbt_project.yml  
â”‚   â”œâ”€â”€ profiles.yml  
â”‚â”€â”€ sql/  
â”‚   â”œâ”€â”€ create_snowflake_schema.sql  
â”‚   â”œâ”€â”€ seed_data.sql  
â”‚â”€â”€ docs/  
â”‚   â”œâ”€â”€ README.md  
â”‚â”€â”€ .env  
â”‚â”€â”€ .gitignore  
â”‚â”€â”€ requirements.txt  
â”‚â”€â”€ setup.sh  
```

## ğŸ›  Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```sh
git clone <your-github-repo-url>
cd customer-analytics-pipeline
```

### 2ï¸âƒ£ Set Up Environment Variables

Create a `.env` file and add the following:

```env
SNOWFLAKE_USER=<your_user>
SNOWFLAKE_PASSWORD=<your_password>
SNOWFLAKE_ACCOUNT=<your_account>
SNOWFLAKE_DATABASE=analytics_db
SNOWFLAKE_WAREHOUSE=compute_wh
POSTGRES_CONN_STRING=postgresql://user:password@host:5432/dbname
```

### 3ï¸âƒ£ Start Airflow with Docker

```sh
docker-compose up -d
```

### 4ï¸âƒ£ Run dbt Setup

```sh
cd dbt
pip install -r requirements.txt
dbt debug
dbt run
```

### 5ï¸âƒ£ Trigger Airflow DAG

- Open Airflow UI at `http://localhost:8080`
- Trigger the `elt_pipeline` DAG manually

## ğŸ“Š Expected Output

1. **Raw transactions** in `raw.transactions`
2. **Transformed customer revenue data** in `analytics.customer_revenue`
3. **Automated daily updates** with Airflow

## ğŸ“Œ Next Steps

- Add **S3 as a staging layer**
- Implement **Airflow Sensors for incremental loads**
- Build **Power BI dashboards on Snowflake**

Happy Data Engineering! ğŸš€

